{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R30H2aJwy99N",
        "outputId": "260c1e3f-2d35-43be-8e32-42c130f7a1c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-12-27 00:17:16--  https://github.com/Infatoshi/fcc-intro-to-llms/blob/main/wizard_of_oz.txt\n",
            "Resolving github.com (github.com)... 192.30.255.113\n",
            "Connecting to github.com (github.com)|192.30.255.113|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 277633 (271K) [text/plain]\n",
            "Saving to: ‘wizard_of_oz.txt’\n",
            "\n",
            "wizard_of_oz.txt    100%[===================>] 271.13K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2023-12-27 00:17:16 (7.01 MB/s) - ‘wizard_of_oz.txt’ saved [277633/277633]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://github.com/Infatoshi/fcc-intro-to-llms/blob/main/wizard_of_oz.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "with open(\"wizard_of_oz.txt\", \"r\", encoding = \"utf-8\") as f:\n",
        "    text = f.read()\n",
        "\n",
        "text = json.loads(text)\n",
        "text = text[\"payload\"][\"blob\"][\"rawLines\"]\n",
        "text = ''.join(line.strip('\\r') for line in text)\n",
        "text[:200]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "CliXkjud2_K4",
        "outputId": "d5299415-cfa8-40b1-de47-3e20dc5ea6fb"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\ufeff  DOROTHY AND THE WIZARD IN OZ  BY  L. FRANK BAUM  AUTHOR OF THE WIZARD OF OZ, THE LAND OF OZ, OZMA OF OZ, ETC.  ILLUSTRATED BY JOHN R. NEILL  BOOKS OF WONDER WILLIAM MORROW & CO., INC. NEW YORK  [Il'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# with open(\"wizard_of_oz.txt\", \"r\", encoding = \"utf-8\") as f:\n",
        "#     text = f.read()\n",
        "\n",
        "print(\"total length of text : \", len(text))\n",
        "print(text[:200])\n",
        "\n",
        "chars = sorted(list(set(text)))\n",
        "stoi = {s:i for i, s in enumerate(chars)}\n",
        "itos = {i:s for s, i in stoi.items()}\n",
        "vocab_size = len(itos)\n",
        "print()\n",
        "print(\"total unique characters : \", vocab_size)\n",
        "\n",
        "encode = lambda s : [stoi[c] for c in s if c in stoi]\n",
        "decode = lambda l : \"\".join([itos[i] for i in l])\n",
        "\n",
        "e = encode(\"Hello World\")\n",
        "d = decode(e)\n",
        "\n",
        "print(e)\n",
        "print(d)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SeklgmvUzMMy",
        "outputId": "6be57530-ccc8-4556-84c4-7e185a35b329"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total length of text :  226887\n",
            "﻿  DOROTHY AND THE WIZARD IN OZ  BY  L. FRANK BAUM  AUTHOR OF THE WIZARD OF OZ, THE LAND OF OZ, OZMA OF OZ, ETC.  ILLUSTRATED BY JOHN R. NEILL  BOOKS OF WONDER WILLIAM MORROW & CO., INC. NEW YORK  [Il\n",
            "\n",
            "total unique characters :  80\n",
            "[31, 57, 64, 64, 67, 0, 46, 67, 70, 64, 56]\n",
            "Hello World\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "data = torch.tensor(encode(text), dtype = torch.long)\n",
        "print(data.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PtY4bZ2izAau",
        "outputId": "b38026b8-6966-4723-903a-6de84fe650f1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([226887])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# HYPER PARAMETER\n",
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
        "\n",
        "device = \"mps\"\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "batch_size = 32\n",
        "block_size = 128\n",
        "learning_rate = 3e-5\n",
        "max_iters = 10000\n",
        "eval_iters = 500\n",
        "dropout = 0.2\n",
        "n_emb = 384\n",
        "n_layer = 16\n",
        "n_head = 16"
      ],
      "metadata": {
        "id": "8T-yE9DA0ErA"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n = int(0.9 * len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "def get_batch(split):\n",
        "    data = train_data if split == \"train\" else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size, ))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x = x.to(device)\n",
        "    y = y.to(device)\n",
        "    return x, y\n",
        "\n",
        "x,y = get_batch(\"train\")\n",
        "print(\"input\")\n",
        "print(x)\n",
        "print(\"targets\")\n",
        "print(y)\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss(model):\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in [\"train\", \"val\"]:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            x, y = get_batch(split)\n",
        "            logits, loss = model(x, y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean().item()\n",
        "    model.train()\n",
        "    return out"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kJ7HHY0G0AIc",
        "outputId": "32f89f46-c9a3-4986-f3be-5a2cc4dfb626"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input\n",
            "tensor([[61, 67, 66,  ...,  0, 77, 67],\n",
            "        [ 0, 71, 73,  ..., 53, 61, 65],\n",
            "        [49, 57, 54,  ..., 68, 68, 57],\n",
            "        ...,\n",
            "        [ 0, 72, 60,  ..., 60,  4, 71],\n",
            "        [70, 53, 66,  ..., 56, 57, 55],\n",
            "        [ 2, 32,  0,  ..., 72, 60, 57]], device='cuda:0')\n",
            "targets\n",
            "tensor([[67, 66, 21,  ..., 77, 67, 73],\n",
            "        [71, 73, 70,  ..., 61, 65, 57],\n",
            "        [57, 54, 10,  ..., 68, 57, 56],\n",
            "        ...,\n",
            "        [72, 60, 57,  ...,  4, 71,  0],\n",
            "        [53, 66, 59,  ..., 57, 55, 64],\n",
            "        [32,  0, 55,  ..., 60, 57,  0]], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class BigramLanguageModel(nn.Module):\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)       # (vocab_size, vocab_size)\n",
        "        self.position_embedding_table = nn.Embedding(vocab_size, vocab_size)    # (vocab_size, vocab_size)\n",
        "\n",
        "    def forward(self, indx, targets=None):                                      # (B, T), (B, T)\n",
        "        logits = self.token_embedding_table(indx)                               # (B, T, vocab_size)\n",
        "        loss = None\n",
        "\n",
        "        if targets is None:\n",
        "            return logits, loss\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)                                        # Since cross entropy accepts the tensors in the shape (B, C, T) we are squeezing the dimentions\n",
        "            targets = targets.view(B*T)                                          # squeezing the targets dimention into 1D\n",
        "\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "            return logits, loss\n",
        "\n",
        "    def generate(self, indx, max_new_tokens):\n",
        "        # index is a (B, T) tensor consisting of the inputs\n",
        "        for _ in range(max_new_tokens):\n",
        "            # get the output from the model\n",
        "            logits, loss = self(indx)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :]                                           # (B, T)\n",
        "            # apply the softmax on the logits to get the probabilities on the last dim\n",
        "            probs = F.softmax(logits, dim=-1)                                   # (B, T)\n",
        "            # choose a index based on the above calculated probability\n",
        "            indx_nxt = torch.multinomial(probs, num_samples=1)                  # (B, 1)\n",
        "            # append the new index to the current index array\n",
        "            indx = torch.cat((indx, indx_nxt), dim=1)                             #(B, T+1)\n",
        "        return indx\n",
        "\n"
      ],
      "metadata": {
        "id": "ACJSomM_0BZj"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = BigramLanguageModel(vocab_size)\n",
        "m = model.to(device)\n",
        "\n",
        "context = torch.zeros((1,1), dtype = torch.long, device=device)\n",
        "generated_chars = decode(m.generate(context, max_new_tokens=500)[0].tolist())\n",
        "print(generated_chars)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PuR0D4N-0JgW",
        "outputId": "e4641b33-1df9-4dd8-8ad3-7df15a81933a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Bf?6FUwHsc]wzIAflBoXgJPdhfA;g5K-7TNBLhPw4kV!&LF&\"EbIXx2JTuRpV8)3r&l.o6,3x[3PdOgmAp)oGSM5GcIUBnlBzSK;YK6UORhNB1lEA6FC﻿jrxN:mJTA1foQ'zIRLhC qx QTD6qO\"(4ph,y'zo1d&R[tFQ\"o1ZKOk4R?oowf)hO8U﻿7q?nC .u*(toTOtQoX7oXj?ewg6gr E;b(TzA1CO4qojh] d8DBN5G-]ngQd.o0VW)LvaEL3Lc35QTNC.!-j_(vO bipFzkk6Bbf)1wAvrwRF&s,[AO,ys0v*1SL.Zg&N5yL 8URdO!h0mn4U5P﻿gvEL*PEbcx!&mCw. Qma-,gPDWV8D2AzAb0.P﻿tJdwI]&49XTxj_LK6MgEp*y&vmS:9\"EFEJWF6c;)LYVlfBzk[l.N,:\"\"E-7BAwfdO10Q﻿:-7_wIncJNZn[KLFVLR3Bl7xc&4Vi\"ELoYNyM9PS.QGH(qs)&8oby1Kf)\"0N\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Now traning the model to generate non random outputs\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr = learning_rate)\n",
        "losses = []\n",
        "\n",
        "for iter in range(max_iters):\n",
        "    # sample a batch of training data\n",
        "    xb, yb = get_batch(\"train\")\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    losses.append(loss.item())\n",
        "\n",
        "    # track stats\n",
        "    if iter % eval_iters == 0:\n",
        "        _loss = estimate_loss(model)\n",
        "        print(f\"{iter} / {max_iters} loss = {_loss}\")\n",
        "print(losses[-1])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IFxXWl4H0LMv",
        "outputId": "f8c03dd9-1556-4f7d-febb-849a26e18c02"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 / 10000 loss = {'train': 4.859058380126953, 'val': 4.8314008712768555}\n",
            "500 / 10000 loss = {'train': 4.834041595458984, 'val': 4.808613300323486}\n",
            "1000 / 10000 loss = {'train': 4.810758113861084, 'val': 4.786744594573975}\n",
            "1500 / 10000 loss = {'train': 4.786906719207764, 'val': 4.764069557189941}\n",
            "2000 / 10000 loss = {'train': 4.765634536743164, 'val': 4.74163818359375}\n",
            "2500 / 10000 loss = {'train': 4.742489814758301, 'val': 4.719244003295898}\n",
            "3000 / 10000 loss = {'train': 4.721006393432617, 'val': 4.696011543273926}\n",
            "3500 / 10000 loss = {'train': 4.698643684387207, 'val': 4.674439430236816}\n",
            "4000 / 10000 loss = {'train': 4.675430774688721, 'val': 4.652345180511475}\n",
            "4500 / 10000 loss = {'train': 4.65268087387085, 'val': 4.6289753913879395}\n",
            "5000 / 10000 loss = {'train': 4.631362438201904, 'val': 4.608608722686768}\n",
            "5500 / 10000 loss = {'train': 4.608999729156494, 'val': 4.585960388183594}\n",
            "6000 / 10000 loss = {'train': 4.587167739868164, 'val': 4.5646748542785645}\n",
            "6500 / 10000 loss = {'train': 4.5641303062438965, 'val': 4.543696880340576}\n",
            "7000 / 10000 loss = {'train': 4.54316520690918, 'val': 4.521167278289795}\n",
            "7500 / 10000 loss = {'train': 4.522913932800293, 'val': 4.50017786026001}\n",
            "8000 / 10000 loss = {'train': 4.500491142272949, 'val': 4.478789806365967}\n",
            "8500 / 10000 loss = {'train': 4.479115009307861, 'val': 4.457760810852051}\n",
            "9000 / 10000 loss = {'train': 4.458024978637695, 'val': 4.437351226806641}\n",
            "9500 / 10000 loss = {'train': 4.437410354614258, 'val': 4.416006088256836}\n",
            "4.432071208953857\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Decoder transformer architecture\n",
        "from tqdm import tqdm\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, n_emb):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_emb, 4 * n_emb),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_emb, n_emb),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_emb, head_size, bias = False)                            # (C, C // head_size)\n",
        "        self.query = nn.Linear(n_emb, head_size, bias = False)                          # (C, C // head_size)\n",
        "        self.value = nn.Linear(n_emb, head_size, bias = False)                          # (C, C // head_suze)\n",
        "        self.register_buffer(\"tril\", torch.tril(torch.ones(block_size, block_size)))\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.shape\n",
        "\n",
        "        # get the 'key' representation\n",
        "        k = self.key(x)             # (B, T, head_size)\n",
        "        # get the 'query' representation\n",
        "        q = self.query(x)           # (B, T, head_size)\n",
        "        # get the 'value' representation\n",
        "        v = self.value(x)           # (B, T, head_size)\n",
        "\n",
        "        # mat mul of key and query\n",
        "        wei = q @ k.transpose(-2, -1) * k.shape[-1] ** -0.5     # (B, T, head_size) @ (B, head_size, T) -> (B, T, T)\n",
        "        # mask fill the key-query pair\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float(\"-inf\"))    # (B, T, T)\n",
        "        # apply softmax\n",
        "        wei = F.softmax(wei, dim=-1)                            # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "\n",
        "        # calculate the out matrix\n",
        "        out = wei @ v               # (B, T, T) @ (B, T, head_size) -> (B, T, head_size)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, n_head, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([SelfAttention(head_size=head_size) for _ in range(n_head)])\n",
        "        self.droupout = nn.Dropout(dropout)\n",
        "        self.proj = nn.Linear(n_head * head_size, n_emb)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # concatenate the output from all the heads\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        # apply dropout\n",
        "        out = self.droupout(out)\n",
        "        # perform a linear transformation\n",
        "        out = self.proj(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, n_emb, n_head):\n",
        "        super().__init__()\n",
        "        # distribute the calculations evenly based on the head size\n",
        "        head_size = n_emb // n_head\n",
        "        self.self_attn = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedForward(n_emb)\n",
        "        self.ln1 = nn.LayerNorm(n_emb)\n",
        "        self.ln2 = nn.LayerNorm(n_emb)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # checkpoint for the residual connection\n",
        "        y = self.self_attn(x)                                   # (B, T, C)\n",
        "        # add the residual checkpoint to the current layer\n",
        "        x = self.ln1(x + y)                                     # (B, T, C)\n",
        "        # checkpoint for the residual connection\n",
        "        y = self.ffwd(x)                                        # (B, T, C)\n",
        "        # add the residual checkpoint to the current layer\n",
        "        x = self.ln2(x + y)                                     # (B, T, C)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "class GPTLanguageModel(nn.Module):\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        # converting each character into a series of embedding eg: 'a' can be represented by [0, 1, 2, ... n_emb]\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_emb)       # (vocab_size, n_emb)\n",
        "        # convertng each position in the embedding table into eg: 0th element can be represented by [0, 0, 0, 0, 0 ... n_emb]\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_emb)    # (vocab_size, vocab_size)\n",
        "        # how many 'Decoder blocks' does this model contain ?\n",
        "        self.blocks = nn.Sequential(*[Block(n_emb, n_head=n_head) for _ in range(n_layer)])\n",
        "        # final layer norm\n",
        "        self.ln_f = nn.LayerNorm(n_emb)\n",
        "        # final linear layer for the language model\n",
        "        self.lm_head = nn.Linear(n_emb, vocab_size)\n",
        "\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean = 0.0, std = 0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean = 0.0, std = 0.02)\n",
        "\n",
        "    def forward(self, indx, targets=None):                                  # (B, T), (B, T)\n",
        "        logits = self.token_embedding_table(indx)                           # (B, T, n_emb)\n",
        "        loss = None\n",
        "\n",
        "        # indx and targets are (B, T) tensor of integers\n",
        "        B, T = indx.shape\n",
        "        # get the token emebddings\n",
        "        tok_emb = self.token_embedding_table(indx)                          # (B, T, n_emb) -> this can also be represented by (B, T, C)\n",
        "        # get the positional embeddings -> rows should have same dimention as 'T' each row is represented by 'n_emb' tensors\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device = device))   # (T, C) -> this can also be represented by (B, T, C)\n",
        "        # Add both positional and token embedding as\n",
        "        x = tok_emb + pos_emb                                               # (B, T, C)\n",
        "        # pass the cocktail of pos and token embedding to the Transformer Blocks\n",
        "        x = self.blocks(x)                                                  # (B, T, C)\n",
        "        # apply layer norm on the final output from the Transformer Blocks\n",
        "        x = self.ln_f(x)                                                    # (B, T, C)\n",
        "        # calcualte the logits by applying a linear transformation\n",
        "        logits = self.lm_head(x)                                            # from (B, T, C {n_emb}) to (B, T, vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            return logits, loss\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)                                        # Since cross entropy accepts the tensors in the shape (B, C, T) we are squeezing the dimentions\n",
        "            targets = targets.view(B*T)                                          # squeezing the targets dimention into 1D\n",
        "\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "            return logits, loss\n",
        "\n",
        "    def generate(self, indx, max_new_tokens):\n",
        "        # index is a (B, T) tensor consisting of the inputs\n",
        "        for _ in tqdm(range(max_new_tokens)):\n",
        "            # crop idx to the last block_size tokens\n",
        "            indx_nxt = indx[:, -block_size:]\n",
        "            # get the output from the model\n",
        "            logits, loss = self(indx_nxt)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :]                                           # (B, T)\n",
        "            # apply the softmax on the logits to get the probabilities on the last dim\n",
        "            probs = F.softmax(logits, dim=-1)                                   # (B, T)\n",
        "            # choose a index based on the above calculated probability\n",
        "            indx_nxt = torch.multinomial(probs, num_samples=1)                  # (B, 1)\n",
        "            # append the new index to the current index array\n",
        "            indx = torch.cat((indx, indx_nxt), dim=1)                             #(B, T+1)\n",
        "        return indx\n",
        "\n",
        "def count_params(model):\n",
        "    s = 0\n",
        "    for p in model.parameters():\n",
        "        if p.requires_grad:\n",
        "            s += p.numel()\n",
        "\n",
        "    return s"
      ],
      "metadata": {
        "id": "DhVrCzAP0M9m"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "model = GPTLanguageModel(vocab_size=vocab_size)\n",
        "m = model.to(device, non_blocking=True)\n",
        "print(device)\n",
        "print(\"Total parameters : \", count_params(model))\n",
        "\n",
        "# Now traning the model to generate non random outputs\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr = learning_rate)\n",
        "losses = []\n",
        "start_time = time.time()\n",
        "\n",
        "for iter in range(max_iters):\n",
        "    # sample a batch of training data\n",
        "    xb, yb = get_batch(\"train\")\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    losses.append(loss.item())\n",
        "\n",
        "    # track stats\n",
        "    if iter % eval_iters == 0:\n",
        "        _loss = estimate_loss(model)\n",
        "        print(f\"{iter} / {max_iters} loss = {_loss} time = {time.time() - start_time}\")\n",
        "        start_time = time.time()\n",
        "\n",
        "print(losses[-1])\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-pv6UQRF0fDq",
        "outputId": "a8297d66-09b6-4ab0-9e72-c146de6e20ed"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n",
            "Total parameters :  28484432\n",
            "0 / 10000 loss = {'train': 4.111767292022705, 'val': 4.122021675109863} time = 206.4224352836609\n",
            "500 / 10000 loss = {'train': 2.4038760662078857, 'val': 2.465719699859619} time = 565.4793167114258\n",
            "1000 / 10000 loss = {'train': 2.2587549686431885, 'val': 2.3316476345062256} time = 569.9537487030029\n",
            "1500 / 10000 loss = {'train': 1.9816352128982544, 'val': 2.0777335166931152} time = 568.9667274951935\n",
            "2000 / 10000 loss = {'train': 1.7736042737960815, 'val': 1.8770356178283691} time = 571.6390836238861\n",
            "2500 / 10000 loss = {'train': 1.640278935432434, 'val': 1.767066478729248} time = 563.2495341300964\n",
            "3000 / 10000 loss = {'train': 1.532592535018921, 'val': 1.6915935277938843} time = 566.9160177707672\n",
            "3500 / 10000 loss = {'train': 1.4463616609573364, 'val': 1.6289525032043457} time = 566.468992471695\n",
            "4000 / 10000 loss = {'train': 1.3776922225952148, 'val': 1.5893322229385376} time = 564.5683524608612\n",
            "4500 / 10000 loss = {'train': 1.316519856452942, 'val': 1.5609573125839233} time = 566.1711263656616\n",
            "5000 / 10000 loss = {'train': 1.264796257019043, 'val': 1.5374977588653564} time = 565.6964452266693\n",
            "5500 / 10000 loss = {'train': 1.2119653224945068, 'val': 1.5165414810180664} time = 568.020298242569\n",
            "6000 / 10000 loss = {'train': 1.1670061349868774, 'val': 1.5105255842208862} time = 569.6288242340088\n",
            "6500 / 10000 loss = {'train': 1.1199225187301636, 'val': 1.497860312461853} time = 567.6098167896271\n",
            "7000 / 10000 loss = {'train': 1.0784162282943726, 'val': 1.505077838897705} time = 565.6126079559326\n",
            "7500 / 10000 loss = {'train': 1.0343564748764038, 'val': 1.4991044998168945} time = 570.6058478355408\n",
            "8000 / 10000 loss = {'train': 0.9894807934761047, 'val': 1.5005528926849365} time = 570.6669766902924\n",
            "8500 / 10000 loss = {'train': 0.9503602385520935, 'val': 1.5156480073928833} time = 566.5343134403229\n",
            "9000 / 10000 loss = {'train': 0.9053921103477478, 'val': 1.530150055885315} time = 565.1648769378662\n",
            "9500 / 10000 loss = {'train': 0.8626407980918884, 'val': 1.5367425680160522} time = 568.2744691371918\n",
            "1.017560601234436\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model, 'model.pt')"
      ],
      "metadata": {
        "id": "9C3LzCIfHs7k"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "model_path = \"./model.pt\"\n",
        "\n",
        "model = torch.load(model_path)\n",
        "model.eval()\n",
        "m = model.to(device)"
      ],
      "metadata": {
        "id": "oFFCSXiMnSYw"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "D7kRdoLwqDXn"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"In the enchanting land of Oz, a spirited young girl named Srinidhi found herself whisked away from her ordinary life into a realm of magic and whimsy. Srinidhi, much like Dorothy in the classic tale, embarked on a fantastical journey where she encountered peculiar characters and faced extraordinary challenges. Accompanied by newfound friends—a wise scarecrow, a compassionate tin woman, and a courageous lion—Srinidhi navigated the vibrant landscapes of Oz, seeking the elusive Wizard who held the key to her return home. Along the Yellow Brick Road, she discovered the strength within herself, overcoming obstacles with resilience and kindness. As the echoes of her adventures reverberated through the Emerald City, Srinidhi's tale became a modern fable of courage, friendship, and the boundless possibilities that unfold when one dares to dream.\"\n",
        "\n",
        "context = torch.tensor(encode(prompt), dtype=torch.long, device=device)\n",
        "generated_chars = decode(m.generate(context.unsqueeze(0), max_new_tokens=1000)[0].tolist())\n",
        "\n",
        "gen_ch_split = 50\n",
        "\n",
        "for i, ch in enumerate(range(len(generated_chars))[:-gen_ch_split:gen_ch_split]):\n",
        "  print(generated_chars[ch:ch+gen_ch_split])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "deUetIhY0jIK",
        "outputId": "7862a7cf-5996-4efa-c329-905f01b71f3a"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "In the enchanting land of Oz, a spirited young gir\n",
            "l named Srinidhi found herself whisked away from h\n",
            "er ordinary life into a realm of magic and whimsy.\n",
            " Srinidhi, much like Dorothy in the classic tale, \n",
            "embarked on a fantastical journey where she encoun\n",
            "tered peculiar characters and faced extraordinary \n",
            "challenges. Accompanied by newfound friendsa wise \n",
            "scarecrow, a compassionate tin woman, and a courag\n",
            "eous lionSrinidhi navigated the vibrant landscapes\n",
            " of Oz, seeking the elusive Wizard who held the ke\n",
            "y to her return home. Along the Yellow Brick Road,\n",
            " she discovered the strength within herself, overc\n",
            "oming obstacles with resilience and kindness. As t\n",
            "he echoes of her adventures reverberated through t\n",
            "he Emerald City, Srinidhi's tale became a modern f\n",
            "able of courage, friendship, and the boundless pos\n",
            "sibilities that unfold when one dares to dream.But\n",
            " they grew stood beginning to remainin the start, \n",
            "and purrise going into the house. How for a few mo\n",
            "ment and the blood weresto any felliw the Gargoyle\n",
            "s, and when the heartilythrewas distance from the \n",
            "creature them had come her was a real decovereda l\n",
            "ook stream their few :\"Don't will me many Magic Be\n",
            "lt, who she began domed in this cavern saw single \n",
            "about to five me about.\"\"Now here,\" said the Wizar\n",
            "d; \"but it was uset it's little?\" asked the Prince\n",
            ". \"We can't be far fillkingin fall together, for I\n",
            " can street you.\"\"And so matter than we stuck you \n",
            "are will hurt, but he made the piglets silent had \n",
            "he was knocked into the air, the Princess dearly l\n",
            "ovings at the ends of the palace intothebut wheels\n",
            " have been to call us a sharp distance. Dorothy wa\n",
            "s not a reton of the station flew the piglets,\"rem\n",
            "arked the Wizard, which was very: \"Well, we must n\n",
            "ear the most people before jurne0ener weselfied 't\n",
            " me to eat.[Illustration: SIN THY LLIThe beautiful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import os\n",
        "import lzma\n",
        "\n",
        "def xz_files_in_dir(directory):\n",
        "    files = []\n",
        "    for filename in os.listdir(directory):\n",
        "        if filename.endswith(\".xz\") and os.path.isfile(os.path.join(directory, filename)):\n",
        "            files.append(filename)\n",
        "    return files\n",
        "\n",
        "folder_path = \"./\"\n",
        "output_file_train = \"output_train.txt\"\n",
        "output_file_val = \"output_val.txt\"\n",
        "vocab_file = \"vocab.txt\"\n",
        "# split_files = int(input(\"How many files would you like to split this into?\"))\n",
        "\n",
        "files = xz_files_in_dir(folder_path)\n",
        "total_files = len(files)\n",
        "\n",
        "# calculate the split index\n",
        "split_index = int(total_files * 0.9)\n",
        "files_train = files[:split_index]\n",
        "files_val = files[split_index:]\n",
        "\n",
        "# process the files for training and validation seperately\n",
        "vocab = set()\n",
        "\n",
        "# process the training files\n",
        "with open(output_file_train, \"w\", encoding=\"utf-8\") as outfile:\n",
        "    for filename in tqdm(files_train, total=len(files_train)):\n",
        "        file_path = os.path.join(folder_path, filename)\n",
        "\n",
        "        with lzma.open(file_path, \"rt\", encoding=\"utf-8\") as infile:\n",
        "            text = infile.read()\n",
        "            outfile.write(text)\n",
        "            characters = set(text)\n",
        "            vocab.update(characters)\n",
        "\n",
        "# process the validation files\n",
        "\n",
        "with open(output_file_val, \"w\", encoding=\"utf-8\") as outfile:\n",
        "    for filename in tqdm(files_val, total=len(files_val)):\n",
        "        file_path = os.path.join(folder_path, filename)\n",
        "\n",
        "        with lzma.open(file_path, \"rt\", encoding=\"utf-8\") as infile:\n",
        "            text = infile.read()\n",
        "            outfile.write(text)\n",
        "            characters = set(text)\n",
        "            vocab.update(characters)\n",
        "\n",
        "# write the vocablury to vocab.txt\n",
        "with open(vocab_file, \"w\", encoding = \"utf-8\") as vfile:\n",
        "    for char in vocab:\n",
        "        vfile.write(char + \"/n\")"
      ],
      "metadata": {
        "id": "WVHYZwR0vB0x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Fk6Rn6O75eD-"
      },
      "execution_count": 13,
      "outputs": []
    }
  ]
}